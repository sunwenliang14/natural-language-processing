{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Biomedical Data position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# pages reference    \n",
    "pages=[0,10,20,30,40,50,60,70,80]\n",
    "\n",
    "# Create dataframe for hodling all info\n",
    "df_bd = pd.DataFrame()    \n",
    "postion_title=[]\n",
    "Position_web=[]\n",
    "Position_date=[]\n",
    "Position_company=[]\n",
    "Position_city=[]\n",
    "Position_state=[]\n",
    "Position_summary=[] \n",
    "\n",
    "# loop each page and scrape each position element by using beautifulSoup \n",
    "for page in pages:        \n",
    "    url = f'https://www.indeed.com/jobs?q=Data+Analyst&l=Texas&sort=date&start={page}'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Scraping each elements\n",
    "    results = soup.find_all(\"h2\", class_='title')\n",
    "    for result in results:    \n",
    "        title=result.a.text.replace('\\n', ' ')\n",
    "        postion_title.append(title)\n",
    "    \n",
    "        link=result.a['href']\n",
    "        web=f\"https://www.indeed.com{link}\" \n",
    "        Position_web.append(web)\n",
    "       \n",
    "    dates = soup.find_all(class_='date')\n",
    "    for date in dates:\n",
    "        update=date.text\n",
    "        Position_date.append(update)\n",
    "    \n",
    "    companies = soup.find_all(class_='company')\n",
    "    for company in companies:\n",
    "        name=company.text.replace('\\n', ' ')\n",
    "        Position_company.append(name)\n",
    "    \n",
    "    locations = soup.find_all(class_='location accessible-contrast-color-location')\n",
    "    for location in locations:\n",
    "        cityname=location.text.split(\",\")[0]\n",
    "        Position_city.append(cityname) \n",
    "        statename=location.text.split(\",\")[-1][0:3]\n",
    "        Position_state.append(statename) \n",
    "    \n",
    "    summaries = soup.find_all(class_='summary')\n",
    "    for summary in summaries:\n",
    "        description=summary.text.replace('\\n', ' ')\n",
    "        Position_summary.append(description)   \n",
    "\n",
    "# add each element into empty dataframe          \n",
    "df_bd[\"Postion\"]=postion_title\n",
    "df_bd[\"Date\"]=Position_date\n",
    "df_bd[\"Company\"]=Position_company\n",
    "df_bd[\"State\"]=Position_state\n",
    "df_bd[\"City\"]=Position_city\n",
    "df_bd[\"Link\"]=Position_web\n",
    "df_bd[\"Description\"]=Position_summary\n",
    "\n",
    "# Export dataframe. \n",
    "df_bd.to_csv(\"Biomedical Data job.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Postion</th>\n",
       "      <th>Date</th>\n",
       "      <th>Company</th>\n",
       "      <th>State</th>\n",
       "      <th>City</th>\n",
       "      <th>Link</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Analyst III, Operations and Maintenance ...</td>\n",
       "      <td>Just posted</td>\n",
       "      <td>Capital Metro</td>\n",
       "      <td>TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=0eb5f97ae7bf3...</td>\n",
       "      <td>Five (5) years of progressively responsible ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Analyst, Corporate Development &amp; Strategy Data</td>\n",
       "      <td>Just posted</td>\n",
       "      <td>U.S. Renal Care</td>\n",
       "      <td>TX</td>\n",
       "      <td>Plano</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=3ea330b6f6674...</td>\n",
       "      <td>Provide data-driven analysis and insights to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Clinical Data Analyst</td>\n",
       "      <td>Just posted</td>\n",
       "      <td>Premier Research Group Limited</td>\n",
       "      <td>TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=4e8d801279c96...</td>\n",
       "      <td>This may include review via data listings, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Performance Data Analyst</td>\n",
       "      <td>Just posted</td>\n",
       "      <td>Harris County</td>\n",
       "      <td>TX</td>\n",
       "      <td>Houston</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=59066ae4c2ba6...</td>\n",
       "      <td>Ensures data quality and integrity in databa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Financial Data Analyst</td>\n",
       "      <td>Just posted</td>\n",
       "      <td>Expedite</td>\n",
       "      <td>TX</td>\n",
       "      <td>Fort Worth</td>\n",
       "      <td>https://www.indeed.com/company/Expediteinc.com...</td>\n",
       "      <td>Audit data integrity across platforms. Coord...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>130</td>\n",
       "      <td>Enterprise BI Data Analyst</td>\n",
       "      <td>6 days ago</td>\n",
       "      <td>DJO Global</td>\n",
       "      <td>TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=90c3d380f2433...</td>\n",
       "      <td>3-5 years’ work experience in a data warehou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>131</td>\n",
       "      <td>Community Data Analyst/Statistician - EPH</td>\n",
       "      <td>6 days ago</td>\n",
       "      <td>Harris County</td>\n",
       "      <td>TX</td>\n",
       "      <td>Pasadena</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=9b137e4f2a0a3...</td>\n",
       "      <td>Coordinates the development, expansion, and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>132</td>\n",
       "      <td>Data Validation Analyst</td>\n",
       "      <td>6 days ago</td>\n",
       "      <td>First Financial Bankshares</td>\n",
       "      <td>TX</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=e269063edee7e...</td>\n",
       "      <td>Responsible for evaluating, identifying, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>Sr. Business Analyst SWHP</td>\n",
       "      <td>6 days ago</td>\n",
       "      <td>Baylor Scott &amp; White Health</td>\n",
       "      <td>TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>https://www.indeed.com/pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>Knowledge of databases, data sources and dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "      <td>Administrative Analyst</td>\n",
       "      <td>6 days ago</td>\n",
       "      <td>City of Lewisville, TX</td>\n",
       "      <td>TX</td>\n",
       "      <td>Lewisville</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=ba577e093868e...</td>\n",
       "      <td>Ability to gather and analyze data and draw ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                            Postion  \\\n",
       "0             0   Data Analyst III, Operations and Maintenance ...   \n",
       "1             1     Analyst, Corporate Development & Strategy Data   \n",
       "2             2                              Clinical Data Analyst   \n",
       "3             3                           Performance Data Analyst   \n",
       "4             4                             Financial Data Analyst   \n",
       "..          ...                                                ...   \n",
       "130         130                         Enterprise BI Data Analyst   \n",
       "131         131          Community Data Analyst/Statistician - EPH   \n",
       "132         132                            Data Validation Analyst   \n",
       "133         133                          Sr. Business Analyst SWHP   \n",
       "134         134                             Administrative Analyst   \n",
       "\n",
       "            Date                           Company State        City  \\\n",
       "0    Just posted                     Capital Metro    TX      Austin   \n",
       "1    Just posted                   U.S. Renal Care    TX       Plano   \n",
       "2    Just posted    Premier Research Group Limited    TX      Austin   \n",
       "3    Just posted                     Harris County    TX     Houston   \n",
       "4    Just posted                          Expedite    TX  Fort Worth   \n",
       "..           ...                               ...   ...         ...   \n",
       "130   6 days ago                        DJO Global    TX      Dallas   \n",
       "131   6 days ago                     Harris County    TX    Pasadena   \n",
       "132   6 days ago        First Financial Bankshares    TX     Abilene   \n",
       "133   6 days ago       Baylor Scott & White Health    TX      Austin   \n",
       "134   6 days ago            City of Lewisville, TX    TX  Lewisville   \n",
       "\n",
       "                                                  Link  \\\n",
       "0    https://www.indeed.com/rc/clk?jk=0eb5f97ae7bf3...   \n",
       "1    https://www.indeed.com/rc/clk?jk=3ea330b6f6674...   \n",
       "2    https://www.indeed.com/rc/clk?jk=4e8d801279c96...   \n",
       "3    https://www.indeed.com/rc/clk?jk=59066ae4c2ba6...   \n",
       "4    https://www.indeed.com/company/Expediteinc.com...   \n",
       "..                                                 ...   \n",
       "130  https://www.indeed.com/rc/clk?jk=90c3d380f2433...   \n",
       "131  https://www.indeed.com/rc/clk?jk=9b137e4f2a0a3...   \n",
       "132  https://www.indeed.com/rc/clk?jk=e269063edee7e...   \n",
       "133  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   \n",
       "134  https://www.indeed.com/rc/clk?jk=ba577e093868e...   \n",
       "\n",
       "                                           Description  \n",
       "0      Five (5) years of progressively responsible ...  \n",
       "1      Provide data-driven analysis and insights to...  \n",
       "2      This may include review via data listings, s...  \n",
       "3      Ensures data quality and integrity in databa...  \n",
       "4      Audit data integrity across platforms. Coord...  \n",
       "..                                                 ...  \n",
       "130    3-5 years’ work experience in a data warehou...  \n",
       "131    Coordinates the development, expansion, and ...  \n",
       "132    Responsible for evaluating, identifying, and...  \n",
       "133    Knowledge of databases, data sources and dat...  \n",
       "134    Ability to gather and analyze data and draw ...  \n",
       "\n",
       "[135 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('Biomedical Data job.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
